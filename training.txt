ppo_networks_configuration = {"trunk_config": {"layer_sizes": [100, 80, 70],
                                      "activations": ["elu",   "elu", "elu"]},

                     "mu_head_config": {"layer_sizes": [60, 50, 3],
                                        "activations": ["elu", "elu", "relu"]},
                     "cov_head_config": {"layer_sizes": [60, 50, 3],
                                        "activations": ["elu","elu", "relu"]},
                     "critic_net_config": {"layer_sizes": [100, 100, 100, 80, 40, 1],
                                            "activations": ["elu", "elu","elu", "elu", "elu", "linear"]},
                     "input_layer_size": 17
                       }


hyperparameters = {"ppo_networks_configuration" : ppo_networks_configuration,
                   "actor_optimizer_mu": tf.keras.optimizers.SGD(learning_rate=0.0001, momentum=0.9),
                   "actor_optimizer_cov": tf.keras.optimizers.SGD(learning_rate=0.0001, momentum=0.9),
                    "critic_optimizer": tf.keras.optimizers.SGD(learning_rate=0.0001, momentum=0.9),
                    "entropy": 1,  
                    "gamma":0.999,
                    "gradient_clipping_actor": 1.0, 
                    "gradient_clipping_critic": 1.0, 
                    "gradient_steps_per_episode_critic": 5,
                    "gradient_steps_per_episode_actor": 10,
                    "epsilon": 0.2,
                    "number_episodes_worker": 10,
                    "n_reward_returns": 5
                    }
agent_config = {
    "action_range": (0, 500),
    "total_number_episodes" : 10000,
    "conwip": 500,
    "run_length": 3000


After 2199 episodes

####Action WIP levels:#####
__Global Agent__
Route 0 : positive between 10 -> 90
Route 1 : -40 -> 80 
Route 2 : positive between 1 -> 4.0 To small


__Mu dist__
Route 0: Healthy
Route 1: Not healthy appears to be always 0
Route 2: Healthy

__Cov dist__
Route 0: Maybe to large for cov values
Route 1: Not healthy, appears to be always 0
Route 2: Healthy

__Worker 0__
Route 0 : -0.4 - 0.4 No change with more episodes
Route 1 : -40 -> 80 
Route 2 : positive between 1 -> 4.0 To small

__Mu dist__
Route 0 : Appears to be all zeros
Route 1 : Healthy 
Route 2 : Not healthy

Same behaviour for the rest of the workers




####Is there something happening with the first unit of the last layer of the neural nets?###

__GRADIENTS__ 

-MU
Gradients more or less healthy most of them very small not an indication of problems for the mu networks

-COV
Same thing as MU


__PARAMETERS__

Other than the bias units weights for all the networks it seems that the distribuition of weights at every layer for MU  and COV 
and CRITIC is not changing. 

__Conclusion__

The network does not appear to be learning anything, some outputs appear to be 0 as well, gradients very close to 0,
it means that for most of the neurons the product sum of weights and subsequent activations is predominantly negative (derivative close to 0)

- Observe gradient logs


	- Mu and Mu_old seem to be very similar at any algorithm cycle, this makes probability ratios be always close to 1 

__Things to try next__

- try larger learning rates 
- Change the activation function shape (ELU)
- Change reward function to include parts produced instead of utilization


__Tests__

-Change output layer activation to ELU from relu. Neurons were apparently dying off. This will not have a effect on throwing
off the actions precribed because of allowing negative values because -the actions are clipped. We can still have negative actions
because of covariances. We can only do this for mu, cov has to be positive since we are modelling a decision agent with a normal
distribution, and normal distributions can not have negative covariances. 
	- Potential solution - Set cov to the absolute value of cov


-Reward is being maximized but wip is not being reduced 
-Change Reward function




-set entropy from 1 to -1: no  result learning very slow to no learning, large covariances
-set entropy back to 1, change learning_rate to 0.001 for every network -> did not work change back to 0.0001 for mu and cov networks
- entropy seems to large on this problem covariance grew to 17.200 wich is unprecedented 
- gradients vanished

Decrease the entropy
  - gradients still vanished
  - actions exploded to the millions 
  
  

